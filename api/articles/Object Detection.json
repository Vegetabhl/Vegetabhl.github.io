{"title":"Object Detection","uid":"633ca7106f3b7f3dd7a3e374f591dcba","slug":"Object Detection","date":"2021-04-23T07:46:28.308Z","updated":"2024-06-07T12:37:49.698Z","comments":true,"path":"api/articles/Object Detection.json","keywords":null,"cover":"https://vegetabhlimg.netlify.app/img/5421e38967a54bd9af308e60506bf081_1.png","content":"<h2 id=\"0-前言\"><a href=\"#0-前言\" class=\"headerlink\" title=\"0 前言\"></a>0 前言</h2><p>以下为读研期间做的一些相关工作，其中one-stage相关模型没有写全，截止毕业时期的sota模型详情请参照毕业论文。</p>\n<p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_01.png\"></p>\n<h2 id=\"1-two-stage\"><a href=\"#1-two-stage\" class=\"headerlink\" title=\"1 two-stage\"></a>1 two-stage</h2><h3 id=\"1-1-RCNN\"><a href=\"#1-1-RCNN\" class=\"headerlink\" title=\"1.1 RCNN\"></a>1.1 RCNN</h3><p>RCNN没写 </p>\n<p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_02.png\"></p>\n<h3 id=\"1-2-Fast-RCNN\"><a href=\"#1-2-Fast-RCNN\" class=\"headerlink\" title=\"1.2 Fast-RCNN\"></a>1.2 Fast-RCNN</h3><h4 id=\"1-2-1-整体结构\"><a href=\"#1-2-1-整体结构\" class=\"headerlink\" title=\"1.2.1 整体结构\"></a>1.2.1 整体结构</h4><p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_03.png\"></p>\n<h4 id=\"1-2-2-算法推理流程\"><a href=\"#1-2-2-算法推理流程\" class=\"headerlink\" title=\"1.2.2 算法推理流程\"></a>1.2.2 算法推理流程</h4><p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_04.png\"></p>\n<p>这里的正负样本是用来训练分类和边界框回归参数的训练数据</p>\n<p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_05.png\"></p>\n<h4 id=\"1-2-3-训练过程-边界框回归损失\"><a href=\"#1-2-3-训练过程-边界框回归损失\" class=\"headerlink\" title=\"1.2.3 训练过程-边界框回归损失\"></a>1.2.3 训练过程-边界框回归损失</h4><p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_06.png\"></p>\n<p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_07.png\"></p>\n<h3 id=\"1-3-Faster-CNN\"><a href=\"#1-3-Faster-CNN\" class=\"headerlink\" title=\"1.3  Faster-CNN\"></a>1.3  Faster-CNN</h3><h4 id=\"1-3-0-RPN\"><a href=\"#1-3-0-RPN\" class=\"headerlink\" title=\"1.3.0 RPN\"></a>1.3.0 RPN</h4><p>参考：</p>\n<p><a href=\"https://www.cnblogs.com/Terrypython/p/10584384.html\">https://www.cnblogs.com/Terrypython/p/10584384.html</a></p>\n<p>这里需要说下，下面这张图上的9是每个特征点对应的anchor个数，也就是分配的9个，<strong>所有的</strong>anchor个数应该等于对应下图的16 * 16 *9。</p>\n<p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_08.png\"></p>\n<p><strong>问题：如果特征图上一个特征点（对应一维特征向量）的感受野小于实际anchor的大小，那还能预测准确吗？比如下图采用VGG作为backbone，那它的实际感受野大小仅为228，但是我最大的anchor已经达到了512。</strong></p>\n<p>​        答：可以，可以用一个小的感受野的特征来近似代替大的anchor的特征，从而完成预测。论文中有提及到，这就像假设平时人眼观察某一物体，只需要观察一部分就可以推断出具体是什么一样。</p>\n<p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_09.png\"></p>\n<p><strong>loss:</strong></p>\n<ul>\n<li><p>N_cls：表示我们采样的正负样本个数，也就是说只有我们采样的正负样本（anchor）才贡献了loss,不是所有的。</p>\n</li>\n<li><p>N_reg:表示所有位置，也就是特征图的W * H</p>\n</li>\n</ul>\n<p>具体loss部分看<strong>1.3.3</strong></p>\n<p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_10.png\"></p>\n<h4 id=\"1-3-1-整体结构\"><a href=\"#1-3-1-整体结构\" class=\"headerlink\" title=\"1.3.1 整体结构\"></a>1.3.1 整体结构</h4><p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_11.png\"></p>\n<h4 id=\"1-3-2-算法推理过程\"><a href=\"#1-3-2-算法推理过程\" class=\"headerlink\" title=\"1.3.2 算法推理过程\"></a>1.3.2 算法推理过程</h4><p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_12.png\"></p>\n<h4 id=\"1-3-3-训练过程\"><a href=\"#1-3-3-训练过程\" class=\"headerlink\" title=\"1.3.3 训练过程\"></a>1.3.3 训练过程</h4><p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_13.png\"></p>\n<p>正负样本机制选取策略（训练阶段）：</p>\n<p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_14.png\"></p>\n<p>这里只写了RPN分类损失，对于边界框回归损失和Fsat-RCNN写的一样</p>\n<p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_15.png\"></p>\n<p>原论文采用的分步训练分步训练RPN网络和Fast-RCNN网络，现在更多的是采用联合训练的办法</p>\n<p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_16.png\"></p>\n<h2 id=\"2-one-stage\"><a href=\"#2-one-stage\" class=\"headerlink\" title=\"2 one-stage\"></a>2 one-stage</h2><h3 id=\"2-1-yolov1\"><a href=\"#2-1-yolov1\" class=\"headerlink\" title=\"2.1 yolov1\"></a>2.1 yolov1</h3><p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_17.png\"></p>\n<p>confidence损失：w，h求损失的时候使用开根号后计算loss。假设对于两个不同尺度大小的gt。假设我们的预测bbox相对gt的偏移量<strong>都相同</strong>。那大尺度的预测效果会更好一些，因为它的iou大嘛。那所有我们肯定是希望效果好的（大目标）loss小一点，效果差的（小目标）loss大一点，所以就用开根号来缓解。看下根号下x的函数图就明白了。</p>\n<p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_18.png\"></p>\n<h3 id=\"2-2-yolov2\"><a href=\"#2-2-yolov2\" class=\"headerlink\" title=\"2.2 yolov2\"></a>2.2 yolov2</h3><p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_19.png\"></p>\n<p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_20.png\"></p>\n<h4 id=\"2-2-1高分辨率分类器\"><a href=\"#2-2-1高分辨率分类器\" class=\"headerlink\" title=\"2.2.1高分辨率分类器\"></a>2.2.1高分辨率分类器</h4><p>之前学习的分类器都是采用224*224大小输入图像进行训练，作者在这里使用448 * 448的输入大小训练得到的分类网络进行迁移学习。</p>\n<p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_21.png\"></p>\n<h4 id=\"2-2-2-直接位置预测\"><a href=\"#2-2-2-直接位置预测\" class=\"headerlink\" title=\"2.2.2 直接位置预测\"></a>2.2.2 直接位置预测</h4><p>那其实可以很容易想到，如果不限制t_x，t_y。一开始的时候因为是随机初始化的参数，那对t_x，t_y的预测就可能非常大，那bbox就不知道飘哪去了。再回过头来通过梯度下降来修正，那这样肯定很不稳定啊，并且多此一举，为什么不直接在一开始就限制t_x，t_y的大小呢。</p>\n<p>所以这就是yolov2关于位置预测的改进：不再使用<strong>直接预测的偏移量</strong>来计算box，而是使用<strong>相对grid cell的偏移量</strong>进行预测。</p>\n<p>这个相对grid cell的偏移量就是对直接预测的偏移量再套个sigmod函数限制。</p>\n<p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_22.png\"></p>\n<h4 id=\"2-2-3-更精细的特征图\"><a href=\"#2-2-3-更精细的特征图\" class=\"headerlink\" title=\"2.2.3 更精细的特征图\"></a>2.2.3 更精细的特征图</h4><p>也就是通过和低层的特征进行融合，作者发现这样可以提升检测<strong>小目标</strong>的效果。</p>\n<p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_23.png\"></p>\n<h4 id=\"2-2-4-多尺度训练\"><a href=\"#2-2-4-多尺度训练\" class=\"headerlink\" title=\"2.2.4 多尺度训练\"></a>2.2.4 多尺度训练</h4><p>增强模型鲁棒性</p>\n<p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_24.png\"></p>\n<p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_25.png\"></p>\n<h3 id=\"2-3-yolov3\"><a href=\"#2-3-yolov3\" class=\"headerlink\" title=\"2.3 yolov3\"></a>2.3 yolov3</h3><p>建议参考链接</p>\n<p><a href=\"https://blog.csdn.net/qq_37963059/article/details/116610443\">https://blog.csdn.net/qq_37963059/article/details/116610443</a></p>\n<p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_26.png\"></p>\n<p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_27.png\"></p>\n<p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_28.png\"></p>\n<p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_29.png\"></p>\n<h3 id=\"2-4-yolov3spp\"><a href=\"#2-4-yolov3spp\" class=\"headerlink\" title=\"2.4 yolov3spp\"></a>2.4 yolov3spp</h3><p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_30.png\"></p>\n<p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_31.png\"></p>\n<p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_32.png\"></p>\n<p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_33.png\"></p>\n<p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_34.png\"></p>\n<p>网络结构：</p>\n<p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_35.png\"></p>\n<h3 id=\"2-5-RetinaNet\"><a href=\"#2-5-RetinaNet\" class=\"headerlink\" title=\"2.5 RetinaNet\"></a>2.5 RetinaNet</h3><p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_36.png\"></p>\n<p><strong>backbone:</strong></p>\n<p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_37.png\"></p>\n<p><strong>head:</strong></p>\n<p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_38.png\"></p>\n<p><strong>loss:</strong></p>\n<p><img src=\"https://vegetabhlimg.netlify.app/img/object_detection_39.png\"></p>\n","feature":true,"text":"0 前言以下为读研期间做的一些相关工作，其中one-stage相关模型没有写全，截止毕业时期的sota模型详情请参照毕业论文。 1 two-stage1.1 R...","permalink":"/post/Object Detection","photos":[],"count_time":{"symbolsCount":"1.6k","symbolsTime":"1 mins."},"categories":[{"name":"Python","slug":"Python","count":3,"path":"api/categories/Python.json"},{"name":"deep learning","slug":"Python/deep-learning","count":3,"path":"api/categories/Python/deep-learning.json"},{"name":"object detection","slug":"Python/deep-learning/object-detection","count":2,"path":"api/categories/Python/deep-learning/object-detection.json"}],"tags":[{"name":"Python","slug":"Python","count":6,"path":"api/tags/Python.json"},{"name":"deep learning","slug":"deep-learning","count":4,"path":"api/tags/deep-learning.json"},{"name":"object detection","slug":"object-detection","count":3,"path":"api/tags/object-detection.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#0-%E5%89%8D%E8%A8%80\"><span class=\"toc-text\">0 前言</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#1-two-stage\"><span class=\"toc-text\">1 two-stage</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#1-1-RCNN\"><span class=\"toc-text\">1.1 RCNN</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#1-2-Fast-RCNN\"><span class=\"toc-text\">1.2 Fast-RCNN</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#1-2-1-%E6%95%B4%E4%BD%93%E7%BB%93%E6%9E%84\"><span class=\"toc-text\">1.2.1 整体结构</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#1-2-2-%E7%AE%97%E6%B3%95%E6%8E%A8%E7%90%86%E6%B5%81%E7%A8%8B\"><span class=\"toc-text\">1.2.2 算法推理流程</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#1-2-3-%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B-%E8%BE%B9%E7%95%8C%E6%A1%86%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1\"><span class=\"toc-text\">1.2.3 训练过程-边界框回归损失</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#1-3-Faster-CNN\"><span class=\"toc-text\">1.3  Faster-CNN</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#1-3-0-RPN\"><span class=\"toc-text\">1.3.0 RPN</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#1-3-1-%E6%95%B4%E4%BD%93%E7%BB%93%E6%9E%84\"><span class=\"toc-text\">1.3.1 整体结构</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#1-3-2-%E7%AE%97%E6%B3%95%E6%8E%A8%E7%90%86%E8%BF%87%E7%A8%8B\"><span class=\"toc-text\">1.3.2 算法推理过程</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#1-3-3-%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B\"><span class=\"toc-text\">1.3.3 训练过程</span></a></li></ol></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#2-one-stage\"><span class=\"toc-text\">2 one-stage</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-1-yolov1\"><span class=\"toc-text\">2.1 yolov1</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-2-yolov2\"><span class=\"toc-text\">2.2 yolov2</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#2-2-1%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E5%88%86%E7%B1%BB%E5%99%A8\"><span class=\"toc-text\">2.2.1高分辨率分类器</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#2-2-2-%E7%9B%B4%E6%8E%A5%E4%BD%8D%E7%BD%AE%E9%A2%84%E6%B5%8B\"><span class=\"toc-text\">2.2.2 直接位置预测</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#2-2-3-%E6%9B%B4%E7%B2%BE%E7%BB%86%E7%9A%84%E7%89%B9%E5%BE%81%E5%9B%BE\"><span class=\"toc-text\">2.2.3 更精细的特征图</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#2-2-4-%E5%A4%9A%E5%B0%BA%E5%BA%A6%E8%AE%AD%E7%BB%83\"><span class=\"toc-text\">2.2.4 多尺度训练</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-3-yolov3\"><span class=\"toc-text\">2.3 yolov3</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-4-yolov3spp\"><span class=\"toc-text\">2.4 yolov3spp</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-5-RetinaNet\"><span class=\"toc-text\">2.5 RetinaNet</span></a></li></ol></li></ol>","author":{"name":"Vegetabhl","slug":"blog-author","avatar":"https://raw.githubusercontent.com/Vegetabhl/Images/master/img/avatar_1.jpg","link":"/","description":"--欢迎来到小卜同学的小破站--     Welcome to Xiaobu’s little website","socials":{"github":"https://github.com/Vegetabhl","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili.svg","link":"https://space.bilibili.com/398361872"},"wechat":{"icon":"/svg/wx.svg","link":"https://raw.githubusercontent.com/Vegetabhl/Images/master/img/wx.jpg"}}}},"mapped":true,"hidden":false,"prev_post":{"title":"Image_Classfication","uid":"d0833af25ffe4b7c183344dd301cd999","slug":"Image_Classfication","date":"2021-04-25T06:40:04.949Z","updated":"2024-06-07T12:40:09.423Z","comments":true,"path":"api/articles/Image_Classfication.json","keywords":null,"cover":"https://vegetabhlimg.netlify.app/img/85e361e1166246d98faaad2591efb1e4_1.png","text":"0 前言以下为读研期间针对分类模型做的一些相关工作。 1 AlexNet 2 VGG 3 GooleNet 4 RestNet 两种残差块 5 MobileNe...","permalink":"/post/Image_Classfication","photos":[],"count_time":{"symbolsCount":592,"symbolsTime":"1 mins."},"categories":[{"name":"Python","slug":"Python","count":3,"path":"api/categories/Python.json"},{"name":"deep learning","slug":"Python/deep-learning","count":3,"path":"api/categories/Python/deep-learning.json"},{"name":"object detection","slug":"Python/deep-learning/object-detection","count":2,"path":"api/categories/Python/deep-learning/object-detection.json"}],"tags":[{"name":"Python","slug":"Python","count":6,"path":"api/tags/Python.json"},{"name":"deep learning","slug":"deep-learning","count":4,"path":"api/tags/deep-learning.json"},{"name":"object detection","slug":"object-detection","count":3,"path":"api/tags/object-detection.json"}],"author":{"name":"Vegetabhl","slug":"blog-author","avatar":"https://raw.githubusercontent.com/Vegetabhl/Images/master/img/avatar_1.jpg","link":"/","description":"--欢迎来到小卜同学的小破站--     Welcome to Xiaobu’s little website","socials":{"github":"https://github.com/Vegetabhl","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili.svg","link":"https://space.bilibili.com/398361872"},"wechat":{"icon":"/svg/wx.svg","link":"https://raw.githubusercontent.com/Vegetabhl/Images/master/img/wx.jpg"}}}},"feature":true},"next_post":{"title":"base_env","uid":"15f9d1d91ee25a75875831eb19af361b","slug":"base_env","date":"2021-01-28T11:41:12.984Z","updated":"2024-12-23T14:37:09.394Z","comments":true,"path":"api/articles/base_env.json","keywords":null,"cover":[],"text":"1 常用命令 列举当前所有环境 1conda env list //显示所有的虚拟环境 创建虚拟环境 1conda create -n your_env_nam...","permalink":"/post/base_env","photos":[],"count_time":{"symbolsCount":"6.2k","symbolsTime":"6 mins."},"categories":[],"tags":[],"author":{"name":"Vegetabhl","slug":"blog-author","avatar":"https://raw.githubusercontent.com/Vegetabhl/Images/master/img/avatar_1.jpg","link":"/","description":"--欢迎来到小卜同学的小破站--     Welcome to Xiaobu’s little website","socials":{"github":"https://github.com/Vegetabhl","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili.svg","link":"https://space.bilibili.com/398361872"},"wechat":{"icon":"/svg/wx.svg","link":"https://raw.githubusercontent.com/Vegetabhl/Images/master/img/wx.jpg"}}}}}}